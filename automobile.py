# -*- coding: utf-8 -*-
"""Automobile.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DlZcKFf_gj8J-VZz9Q8KCh5b2NKPmoz-
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

df = pd.read_csv("/content/Automobile_data.csv")
df.head()

"""check for number of rows and columns"""

# Check the number of rows and columns
num_rows,num_columns= df.shape
print(f"number of rows: {num_rows}")
print(f"number of columns : {num_columns}")

"""check the datatypes of each column"""

# Data types of each column
data_types = df.dtypes
print("Data types of each column:")
print(data_types)

# Summary statistics for numeric columns
numeric_summary = df.describe()
print("Summary statistics for numeric columns:")
print(numeric_summary.to_string())

# describe all the columns in "df"
df.describe(include = "all")

df.replace("?",np.nan, inplace=True)
df.head(5)

m=df.isnull()
m.head(5)

missing_values = df.isnull().sum()
print("Missing values in each column:")
print(missing_values)

# Replace missing values in these columns with their respective averages
columns_to_impute = ["normalized-losses", "bore", "stroke", "horsepower", "peak-rpm"]

for column in columns_to_impute:
    # Calculate the average of the current column
    avg_value = df[column].astype("float").mean(axis=0)

    # Replace missing values in the current column with the calculated average
    df[column].replace(np.nan, avg_value, inplace=True)

# Display the calculated average values
for column in columns_to_impute:
    avg_value = df[column].astype("float").mean(axis=0)
    print(f"Average of {column}: {avg_value}")

# Replace missing values in the 'num-of-doors' column with the most frequent value ('four')
df['num-of-doors'].replace(np.nan, "four", inplace=True)

# Drop rows with missing values in the 'price' column
df.dropna(subset=["price"], axis=0, inplace=True)

# Reset the DataFrame index after dropping rows
df.reset_index(drop=True, inplace=True)

df.head()

"""converting all values to proper data types"""

# Define a dictionary to map columns to their desired data types
data_types = {
    "bore": float,
    "stroke": float,
    "normalized-losses": int,
    "price": float,
    "peak-rpm": float
}

# Convert the specified columns to their respective data types
df = df.astype(data_types)

# Print the data types of the columns to verify the conversion
print(df.dtypes)

# # Iterate through the "horsepower" column and print non-numeric values
# for value in df["horsepower"]:
#     try:
#         # Attempt to convert the value to a numeric type
#         print(value)
#         float(value)

#     except (ValueError, TypeError):
#         # If an exception is raised, the value is non-numeric
#         print(f"Non-numeric value in 'horsepower' column: {value}")

# Convert "horsepower" column to int, handling non-numeric values
df["horsepower"] = pd.to_numeric(df["horsepower"], errors="coerce").astype("float64")

# Print the data type of the "horsepower" column after conversion
print("Data type of 'horsepower' column after conversion:", df["horsepower"].dtype)

df["city-L/100km"] = 235 / df["city-mpg"]

df.head()

df["highway-mpg"] = 235/df["highway-mpg"]
df.rename(columns={'"highway-mpg"':'highway-L/100km'}, inplace=True)

# Select the numeric columns you want to visualize, excluding 'price'
numeric_columns = df.select_dtypes(include=['int64', 'float64']).drop(columns=['price'])

# Create box plots and histograms for each numeric column individually
for column in numeric_columns.columns:
    plt.figure(figsize=(6, 4))

    # Create a box plot
    plt.subplot(1, 2, 1)
    sns.boxplot(data=df, y=column)
    plt.title(f"Box Plot of {column}")

    # Create a histogram
    plt.subplot(1, 2, 2)
    sns.histplot(data=df, x=column, kde=True, bins=20)
    plt.title(f"Histogram of {column}")

    plt.tight_layout()
    plt.show()

"""from the above graphs we can say that we need standardization"""

df.head(2)

df["horsepower"] = df["horsepower"].astype(int,copy=True)

# Commented out IPython magic to ensure Python compatibility.
# Convert the "horsepower" column to integer (if needed)
df["horsepower"] = df["horsepower"].astype(int, errors="ignore")

# Enable inline plotting
# %matplotlib inline

# Create a histogram of the "horsepower" column
plt.hist(df["horsepower"], bins=20, edgecolor="k")  # Adjust the number of bins as needed

plt.xlabel("Horsepower")
plt.ylabel("Count")
plt.title("Horsepower Distribution")

plt.show()

bins = np.linspace(min(df["horsepower"]),max(df["horsepower"]),4)
bins

group_names = ['Low', 'Medium', 'High']

df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names ,include_lowest = True)
df[['horsepower','horsepower-binned']].head(20)

# Count the occurrences of each bin and create a bar plot
plt.bar(group_names, df["horsepower-binned"].value_counts())

# Set x/y labels and plot title
plt.xlabel("Horsepower")
plt.ylabel("Count")
plt.title("Horsepower Bins")

# Show the plot
plt.show()

# Create a histogram with 3 bins for the "horsepower" column
plt.hist(df["horsepower"], bins=3, edgecolor="k")

# Set x/y labels and plot title
plt.xlabel("Horsepower")
plt.ylabel("Count")
plt.title("Horsepower Bins")

# Show the plot
plt.show()

dummy_variable_1 = pd.get_dummies(df["fuel-type"])
dummy_variable_1.head()

dummy_variable_1.rename(columns={'fuel-type-diesel':'gas', 'fuel-type-diesel':'diesel'}, inplace=True)
dummy_variable_1.head()

# merge data frame "df" and "dummy_variable_1"
df = pd.concat([df, dummy_variable_1], axis=1)

# drop original column "fuel-type" from "df"
df.drop("fuel-type", axis = 1, inplace=True)

dummy_variable_2 = pd.get_dummies(df["aspiration"])
dummy_variable_2.rename(columns={'std':"aspiration-std",'turbo':"aspiration-turbo"},inplace=True)
dummy_variable_2.head()

df = pd.concat([df,dummy_variable_2],axis=1)
df.drop('aspiration',axis=1,inplace=True)

df.to_csv('clean_df.csv')

df = pd.read_csv("/content/clean_df.csv")
df.head()

import seaborn as sns

df.corr()

# Select the columns of interest
columns_of_interest = ['bore', 'stroke', 'compression-ratio', 'horsepower']

# Calculate the correlation matrix
correlation_matrix = df[columns_of_interest].corr()

# Display the correlation matrix
print(correlation_matrix)

# List of numerical columns (excluding 'price')
numerical_columns = ["engine-size", "horsepower", "highway-mpg", "curb-weight","peak-rpm","stroke"]  # Add or remove columns as needed

# Iterate through numerical columns
for column in numerical_columns:
    # Create a regression plot for the current column vs. "price"
    sns.lmplot(x=column, y="price", data=df)
    plt.title(f"{column} vs. Price")

    # Show the plot
    plt.show()

    # Calculate and display the correlation coefficient
    correlation_coefficient = df[[column, 'price']].corr().iloc[0, 1]
    print(f"Correlation coefficient between {column} and price: {correlation_coefficient}")

# List of categorical columns
categorical_columns = ["body-style", "engine-location", "drive-wheels"]

# Create subplots
fig, axes = plt.subplots(nrows=len(categorical_columns), ncols=1, figsize=(8, 12))

# Iterate through categorical columns and create box plots
for i, column in enumerate(categorical_columns):
    sns.boxplot(x=column, y="price", data=df, ax=axes[i])
    axes[i].set_title(f"{column} vs. Price")
    axes[i].set_xlabel(column)
    axes[i].set_ylabel("Price")

# Adjust spacing between subplots
plt.tight_layout()

# Show the plots
plt.show()

df.describe()

df.describe(include=['object'])

df['drive-wheels'].value_counts()

df['drive-wheels'].value_counts().to_frame()

drive_wheels_counts = df['drive-wheels'].value_counts().to_frame()
drive_wheels_counts.rename(columns={'drive-wheels':'value_counts'},inplace=True)
drive_wheels_counts

drive_wheels_counts.index.name = "drive_wheels"
drive_wheels_counts

engine_loc_counts = df.groupby('engine-location').size().reset_index(name='value_counts')
engine_loc_counts.set_index('engine-location', inplace=True)
engine_loc_counts.head(10)

df_group_one = df[['drive-wheels','body-style','price']]

df_group_one = df[['drive-wheels', 'body-style', 'price']].groupby(['drive-wheels'], as_index=False).mean()
df_group_one

grouped_test1 = df[['drive-wheels', 'body-style', 'price']].groupby(['drive-wheels', 'body-style'], as_index=False).mean()
grouped_test1

grouped_pivot = df.pivot_table(index='drive-wheels', columns='body-style', values='price', aggfunc='mean')
grouped_pivot

grouped_pivot = grouped_pivot.fillna(0) #fill missing values with 0
grouped_pivot

grouped_test_bodystyle = df[['body-style', 'price']].groupby(['body-style'], as_index=False).mean()
grouped_test_bodystyle

#use the grouped results
plt.pcolor(grouped_pivot, cmap='RdBu')
plt.colorbar()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Create a pivot table
grouped_pivot = df.pivot_table(index='drive-wheels', columns='body-style', values='price', aggfunc='mean')

# Create a heatmap using seaborn
plt.figure(figsize=(10, 6))
sns.heatmap(grouped_pivot, annot=True, cmap='RdBu')

# Set axis labels and plot title
plt.xlabel('Body Style')
plt.ylabel('Drive Wheels')
plt.title('Price Heatmap')

plt.show()

from scipy import stats

numeric_columns = ['wheel-base', 'horsepower', 'length', 'width', 'curb-weight', 'engine-size', 'bore', 'city-mpg', 'highway-mpg']

for column in numeric_columns:
    pearson_coef, p_value = stats.pearsonr(df[column], df['price'])
    print(f"The Pearson Correlation Coefficient between {column} and price is {pearson_coef} with a P-value of {p_value}")

df_gptest = df[['drive-wheels', 'price']]
grouped_test2 = df_gptest.groupby(['drive-wheels'])
grouped_test2.head(2)

df_gptest

four_wheel_drive_prices = grouped_test2.get_group('4wd')['price']
print(four_wheel_drive_prices)

"""ANOVA"""

# List of unique drive-wheels values
drive_wheels = df['drive-wheels'].unique()

# Initialize empty lists to store results
f_values = []
p_values = []

# Calculate ANOVA for each drive-wheels group
for drive_wheel in drive_wheels:
    group = grouped_test2.get_group(drive_wheel)['price']
    f_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], group)
    f_values.append(f_val)
    p_values.append(p_val)

# Print ANOVA results
for i, drive_wheel in enumerate(drive_wheels):
    print(f"ANOVA results for {drive_wheel}: F={f_values[i]}, P={p_values[i]}")

"""Machine learning model"""

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Select the independent variable (feature) and the target variable
X = df[['engine-size']]  # Feature
y = df['price']          # Target

# Create a scatter plot of the data
plt.scatter(X, y, alpha=0.5)
plt.xlabel('Engine Size')
plt.ylabel('Price')
plt.title('Scatter Plot of Engine Size vs. Price')
plt.show()

# Select the independent variable (feature) and the target variable
X = df[['engine-size']]  # Feature
y = df['price']          # Target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Fit the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Calculate residuals
residuals = y_test - y_pred

# Create a histogram of residuals
plt.hist(residuals, bins=30, edgecolor='k')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.title('Histogram of Residuals')
plt.show()

import statsmodels.api as sm
import scipy.stats as stats

# Create a Q-Q plot
sm.qqplot(residuals, line='s')
plt.title('Normal Probability Plot (Q-Q Plot) of Residuals')
plt.show()

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Plot the regression line
plt.scatter(X_test, y_test, color='blue')
plt.plot(X_test, y_pred, color='red', linewidth=3)
plt.xlabel('Engine Size')
plt.ylabel('Price')
plt.title('Simple Linear Regression')
plt.show()

import statsmodels.api as sm

# Add a constant term to the X matrix
X_train_const = sm.add_constant(X_train)

# Fit a linear regression model
model = sm.OLS(y_train, X_train_const).fit()

# Get the summary statistics of the model
model_summary = model.summary()

# Print the summary to see t-statistics, p-values, etc.
print(model_summary)

# Select multiple independent variables (features) and the target variable
X_multi = df[['engine-size', 'horsepower', 'curb-weight','highway-mpg']]  # Features
y_multi = df['price']                                      # Target

# Split the data into training and testing sets
X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)

# Create a multiple linear regression model
model_multi = LinearRegression()
# Fit the model on the training data
model_multi.fit(X_train_multi, y_train_multi)

# Make predictions on the test data
y_pred_multi = model_multi.predict(X_test_multi)

# Calculate the residuals
residuals_multi = y_test_multi - y_pred_multi

# Create a scatter plot of predicted values vs. residuals
plt.scatter(y_pred_multi, residuals_multi, c='b', s=40, alpha=0.5)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values for Multiple Linear Regression")
plt.axhline(y=0, color='r', linestyle='--')
plt.show()

import scipy.stats as stats
import matplotlib.pyplot as plt

# Calculate the residuals
residuals_multi = y_test_multi - y_pred_multi

# Create a Q-Q plot
stats.probplot(residuals_multi, dist="norm", plot=plt)
plt.title("Q-Q Plot of Residuals for Multiple Linear Regression")
plt.xlabel("Theoretical Quantiles")
plt.ylabel("Sample Quantiles")
plt.show()

# Evaluate the model
mse_multi = mean_squared_error(y_test_multi, y_pred_multi)
r2_multi = r2_score(y_test_multi, y_pred_multi)

print("Mean Squared Error (Multiple Linear Regression):", mse_multi)
print("R-squared (Multiple Linear Regression):", r2_multi)

# Make predictions using your multiple linear regression model
Yhat = model_multi.predict(X_multi)

# Create a distribution plot
plt.figure(figsize=(10, 6))
ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Values")
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values", ax=ax1)

plt.title("Actual vs Fitted Values for Price")
plt.xlabel("Price (in dollars)")
plt.ylabel("Proportion of Cars")
plt.legend()
plt.show()

from sklearn.feature_selection import f_regression
f_stats, p_values = f_regression(X_multi, y_multi)

for i, feature in enumerate(X_multi.columns):
    print(f"Feature: {feature}")
    print(f"F-statistic: {f_stats[i]}")
    print(f"P-value: {p_values[i]}")
    if p_values[i] < 0.05:
        print("This feature is statistically significant.")
    else:
        print("This feature is not statistically significant.")
    print("=" * 30)

from statsmodels.api import OLS
from statsmodels.tools import add_constant

# Add a constant term to the independent variables
X_multi_const = add_constant(X_multi)

# Fit the multiple linear regression model
model = OLS(y_multi, X_multi_const).fit()

# Get the t-statistics and p-values for each coefficient
t_stats = model.tvalues
p_values = model.pvalues

# Print the t-statistics and p-values
for i, feature in enumerate(X_multi_const.columns):
    print(f"Feature: {feature}")
    print(f"T-statistic: {t_stats[i]}")
    print(f"P-value: {p_values[i]}")
    print("---------------------------")